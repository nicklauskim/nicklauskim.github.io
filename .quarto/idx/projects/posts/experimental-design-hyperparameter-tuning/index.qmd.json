{"title":"Hyperparameter Tuning for Gradient Boosting Frameworks Using Statistical Experimental Design","markdown":{"yaml":{"title":"Hyperparameter Tuning for Gradient Boosting Frameworks Using Statistical Experimental Design","description":"Applying Advanced Experimental Designs to Optimization of Hyperparameters in XGBoost","date":"2025-05-10","categories":["statistics","bayesian","experimental design"],"image":"images/test_tubes.jpg"},"headingText":"Introduction/Background","containsRefs":false,"markdown":"\n\n\nExperimental design is a crucial part of any statistical experiment, including computer simulations. To explore the eﬀects of eﬃcient design on a computer experiment, we examine three published scientific papers and repeat the papers’ experiments with alternative methods of analysis using advanced experimental designs such as Latin Hypercube Sampling (LHS) and Maximum Projection Designs.\n\nIn the computer simulations discussed here, the experiments at hand are the machine learning models themselves, with the predictive performance of the model serving as the output and the various hyperparameters serving as inputs. A common theme throughout statistical and machine learning research papers -- and all the computational papers we examine here -- is that the authors implement an expensive grid search to optimize the model parameters. However, this approach can be very computationally expensive, both in terms of model run times and computing resources. Moreover, choosing hyperparameters in this way does not attempt to model or account for the relationships between the model performance and hyperparameters. We would like to address these shortcomings in our analysis.\n\nFor this project, we keep a few key goals in mind. First of all, we would like to achieve better predictive performance than what was reported in each of the individual papers, which are geared towards computational physics and chemistry and not super statistically fine-tuned for the utmost optimal model results. Secondly, we look to reduce the estimated run time to save on computational resources. And lastly, we seek to produce a systematic framework with which one can continually improve model performance. As for our methodology, we incorporate principles of computer experiment design in conjunction with Gaussian Process (GP) modeling, with more details presented in the Appendix section below.\n\nIn each paper, the authors use a common method: XGBoost (Chen et al. 2016). For a quick description of the model parameters see (Chen et al. 2018a). In the remainder of this report we discuss each of the investigated research papers in turn, comparing their experimental design methodology and final model results to that of our analysis.\n\n\n## Critical Temperatures of Superconductors\nThe first paper we analyze is “A Data-Driven Statistical Model for Predicting the Critical Temperature of a Superconductor”, published in the Journal of Computational Material Science. The author’s objective in this first paper is to predict at which temperature amaterial becomes a superconductor. A good use case for modeling this relationship is to pre-screen various compounds and find their critical temperatures. There is currently no widely accepted physical theory for the critical temperature of a superconductor, so using a statistical model is a good alternative to model the behavior.[^1]\n\n[^1]: A superconductor is classified as any material which can transport electric charge with no resistance or\nwith no energy loss.\n\nIn the dataset that the author used and provided, the outcome variable is the critical temperature and the predictor variables are various physical properties of the material, such as atomic mass, atomic radius, etc. The data has 21,263 observations in total with 82 columns. For their analysis, the author conducted an exhaustive 198-point grid search over five hyperparameters: learning rate, column subsampling, row subsampling, minimum\nnodes, and maximum depth (Table 1). This amounts to taking all possible combinations of their chosen level settings. For performance evaluation, they used a 25-fold Monte Carlo\ncross-validation (CV) using root mean square error (RMSE) as the performance metric. For each iteration they split the data in two thirds for model fitting and one third for model\nevaluation.\n\n::: {.table-narrow style=\"width: 70%; margin: auto;\"}\n| Parameters         | Levels              |\n|--------------------|---------------------|\n| Learning rate      | 0.010, 0.015, 0.020 |\n| Column subsampling | 0.25, 0.50, 0.75    |\n| Subsample ratio    | 0.5                 |\n| Minimum nodes      | 1, 10               |\n| Maximum depth      | 15, 16, ..., 25     |\n<div style=\"text-align: center;\">**Table 1: Superconductors Hyperparameter Levels**</div>\n:::\n<br>\n\nFor our analysis, we construct a Maximum Projection Latin Hypercube with 10 data points (using the MaxPro package in R) and use the same evaluation procedure as the author of the original paper. To transform samples from our design to integers, for discrete hyperparameters such as maximum depth (number of leaves) of trees, we apply the samples to the inverse CDF of the uniform distribution and take the ceiling of the resulting values. To transform the continuous hyperparameters from our design into the appropriate range we can simply shift them by a linear transformation. (We repeat this procedure for all three papers.)\n\nWe can visualize the comparison of all two-dimensional projections of the design that we generate and the design used by the author (Figure 1). From the plots we can see that the author’s design is not space filling in all of its projections, however our design does enjoy nice space filling and projection properties. Some of the projections of the author’s design are absurd, such as the max depth and min node - discrete parameters - which project as a continuous line. Although some of the author’s parameters are uniformly distributed, like maximum depth, this is not the case for all of their parameters, especially for the continuous parameters like learning rate.\n\n![](images/paper_1_all_2D_projections.png)\n<div style=\"text-align: center;\">*Figure 1:* Two-Dimensional Hyperparameter Projections. Author's Design in Black. Our Design in Red.</div>\n\nFor the Gaussian Process (GP) Model, we use the Matern (5/2) covariance function with a small noise term added and linear mean functions. We introduce the noise term due to the fact that parameters like row and column subsampling are inherently random and lead to diﬀerent performance evaluation. The estimated GP parameters can be seen in Table 2 and Table 3 below. Then, we run 10 iterations of the Expectation Globalization Optimization (EGO) algorithm. We can see from that after only a couple iterations, the gradient boosting model is performing at a lower RMSE than the author’s final reported value (Figure 2). In the end we arrive at an entirely diﬀerent set of parameters than the author's set (Table 4). Further, the computation cost of running the model from start to finish is drastically reduced. In this instance, we are able to achieve better performance in approximately 1/10th of the run time for the author’s experiment. We estimate the author’s run time by taking the average run time on our 12 core desktop computer and multiplying by the number of configurations the author used to find hyperparamter values.\n\n::: {.table-narrow style=\"width: 70%; margin: auto;\"}\n| Parameters         | Estimates           |\n|--------------------|---------------------|\n| (Intercept)        | 88.299              |\n| Learning rate      | 1.532               |\n| Subsample ratio    | 0.4                 |\n| Minimum nodes      | -2.56               |\n| Maximum depth      | 0.490               |\n<div style=\"text-align: center;\">**Table 2: Gaussian Process Parameters: Trend Coefficients**</div>\n:::\n<br>\n\n::: {.table-narrow style=\"width: 70%; margin: auto;\"}\n| Parameters             | Estimates           |\n|------------------------|---------------------|\n| Theta(Learning rate)   | 0.455               |\n| Theta(Subsample ratio) | 1.800               |\n| Theta(Minimum nodes)   | 0.164               |\n| Theta(Maximum depth)   | 1.800               |\n<div style=\"text-align: center;\">**Table 3: Gaussian Process Parameters: Covariance**</div>\n:::\n<br>\n\n![](images/super_conductor_ego.png)\n<div style=\"text-align: center;\">*Figure 2:* EGO Algorithm Results</div>\n\n::: {.table-narrow style=\"width: 70%; margin: auto;\"}\n|                    | Author's Results    | Our Results         |\n|--------------------|---------------------|---------------------|\n| Learning rate      | 0.016               | 0.029               |\n| Subsample ratio    | 0.400               | 0.200               |\n| Minimum nodes      | 10                  | 11                  |\n| Maximum depth      | 16                  | 10                  |\n| RMSE               | 44.090              | 41.911              |\n| Estimated Runtime  | 2 hours             | 8 minutes           |\n<div style=\"text-align: center;\">**Table 4: Final XGBoost Model Comparison**</div>\n:::\n<br>\n\nTo be continued...\n\n","srcMarkdownNoYaml":"\n\n\n## Introduction/Background\nExperimental design is a crucial part of any statistical experiment, including computer simulations. To explore the eﬀects of eﬃcient design on a computer experiment, we examine three published scientific papers and repeat the papers’ experiments with alternative methods of analysis using advanced experimental designs such as Latin Hypercube Sampling (LHS) and Maximum Projection Designs.\n\nIn the computer simulations discussed here, the experiments at hand are the machine learning models themselves, with the predictive performance of the model serving as the output and the various hyperparameters serving as inputs. A common theme throughout statistical and machine learning research papers -- and all the computational papers we examine here -- is that the authors implement an expensive grid search to optimize the model parameters. However, this approach can be very computationally expensive, both in terms of model run times and computing resources. Moreover, choosing hyperparameters in this way does not attempt to model or account for the relationships between the model performance and hyperparameters. We would like to address these shortcomings in our analysis.\n\nFor this project, we keep a few key goals in mind. First of all, we would like to achieve better predictive performance than what was reported in each of the individual papers, which are geared towards computational physics and chemistry and not super statistically fine-tuned for the utmost optimal model results. Secondly, we look to reduce the estimated run time to save on computational resources. And lastly, we seek to produce a systematic framework with which one can continually improve model performance. As for our methodology, we incorporate principles of computer experiment design in conjunction with Gaussian Process (GP) modeling, with more details presented in the Appendix section below.\n\nIn each paper, the authors use a common method: XGBoost (Chen et al. 2016). For a quick description of the model parameters see (Chen et al. 2018a). In the remainder of this report we discuss each of the investigated research papers in turn, comparing their experimental design methodology and final model results to that of our analysis.\n\n\n## Critical Temperatures of Superconductors\nThe first paper we analyze is “A Data-Driven Statistical Model for Predicting the Critical Temperature of a Superconductor”, published in the Journal of Computational Material Science. The author’s objective in this first paper is to predict at which temperature amaterial becomes a superconductor. A good use case for modeling this relationship is to pre-screen various compounds and find their critical temperatures. There is currently no widely accepted physical theory for the critical temperature of a superconductor, so using a statistical model is a good alternative to model the behavior.[^1]\n\n[^1]: A superconductor is classified as any material which can transport electric charge with no resistance or\nwith no energy loss.\n\nIn the dataset that the author used and provided, the outcome variable is the critical temperature and the predictor variables are various physical properties of the material, such as atomic mass, atomic radius, etc. The data has 21,263 observations in total with 82 columns. For their analysis, the author conducted an exhaustive 198-point grid search over five hyperparameters: learning rate, column subsampling, row subsampling, minimum\nnodes, and maximum depth (Table 1). This amounts to taking all possible combinations of their chosen level settings. For performance evaluation, they used a 25-fold Monte Carlo\ncross-validation (CV) using root mean square error (RMSE) as the performance metric. For each iteration they split the data in two thirds for model fitting and one third for model\nevaluation.\n\n::: {.table-narrow style=\"width: 70%; margin: auto;\"}\n| Parameters         | Levels              |\n|--------------------|---------------------|\n| Learning rate      | 0.010, 0.015, 0.020 |\n| Column subsampling | 0.25, 0.50, 0.75    |\n| Subsample ratio    | 0.5                 |\n| Minimum nodes      | 1, 10               |\n| Maximum depth      | 15, 16, ..., 25     |\n<div style=\"text-align: center;\">**Table 1: Superconductors Hyperparameter Levels**</div>\n:::\n<br>\n\nFor our analysis, we construct a Maximum Projection Latin Hypercube with 10 data points (using the MaxPro package in R) and use the same evaluation procedure as the author of the original paper. To transform samples from our design to integers, for discrete hyperparameters such as maximum depth (number of leaves) of trees, we apply the samples to the inverse CDF of the uniform distribution and take the ceiling of the resulting values. To transform the continuous hyperparameters from our design into the appropriate range we can simply shift them by a linear transformation. (We repeat this procedure for all three papers.)\n\nWe can visualize the comparison of all two-dimensional projections of the design that we generate and the design used by the author (Figure 1). From the plots we can see that the author’s design is not space filling in all of its projections, however our design does enjoy nice space filling and projection properties. Some of the projections of the author’s design are absurd, such as the max depth and min node - discrete parameters - which project as a continuous line. Although some of the author’s parameters are uniformly distributed, like maximum depth, this is not the case for all of their parameters, especially for the continuous parameters like learning rate.\n\n![](images/paper_1_all_2D_projections.png)\n<div style=\"text-align: center;\">*Figure 1:* Two-Dimensional Hyperparameter Projections. Author's Design in Black. Our Design in Red.</div>\n\nFor the Gaussian Process (GP) Model, we use the Matern (5/2) covariance function with a small noise term added and linear mean functions. We introduce the noise term due to the fact that parameters like row and column subsampling are inherently random and lead to diﬀerent performance evaluation. The estimated GP parameters can be seen in Table 2 and Table 3 below. Then, we run 10 iterations of the Expectation Globalization Optimization (EGO) algorithm. We can see from that after only a couple iterations, the gradient boosting model is performing at a lower RMSE than the author’s final reported value (Figure 2). In the end we arrive at an entirely diﬀerent set of parameters than the author's set (Table 4). Further, the computation cost of running the model from start to finish is drastically reduced. In this instance, we are able to achieve better performance in approximately 1/10th of the run time for the author’s experiment. We estimate the author’s run time by taking the average run time on our 12 core desktop computer and multiplying by the number of configurations the author used to find hyperparamter values.\n\n::: {.table-narrow style=\"width: 70%; margin: auto;\"}\n| Parameters         | Estimates           |\n|--------------------|---------------------|\n| (Intercept)        | 88.299              |\n| Learning rate      | 1.532               |\n| Subsample ratio    | 0.4                 |\n| Minimum nodes      | -2.56               |\n| Maximum depth      | 0.490               |\n<div style=\"text-align: center;\">**Table 2: Gaussian Process Parameters: Trend Coefficients**</div>\n:::\n<br>\n\n::: {.table-narrow style=\"width: 70%; margin: auto;\"}\n| Parameters             | Estimates           |\n|------------------------|---------------------|\n| Theta(Learning rate)   | 0.455               |\n| Theta(Subsample ratio) | 1.800               |\n| Theta(Minimum nodes)   | 0.164               |\n| Theta(Maximum depth)   | 1.800               |\n<div style=\"text-align: center;\">**Table 3: Gaussian Process Parameters: Covariance**</div>\n:::\n<br>\n\n![](images/super_conductor_ego.png)\n<div style=\"text-align: center;\">*Figure 2:* EGO Algorithm Results</div>\n\n::: {.table-narrow style=\"width: 70%; margin: auto;\"}\n|                    | Author's Results    | Our Results         |\n|--------------------|---------------------|---------------------|\n| Learning rate      | 0.016               | 0.029               |\n| Subsample ratio    | 0.400               | 0.200               |\n| Minimum nodes      | 10                  | 11                  |\n| Maximum depth      | 16                  | 10                  |\n| RMSE               | 44.090              | 41.911              |\n| Estimated Runtime  | 2 hours             | 8 minutes           |\n<div style=\"text-align: center;\">**Table 4: Final XGBoost Model Comparison**</div>\n:::\n<br>\n\nTo be continued...\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.42","theme":["cosmo","brand"],"toggle":true,"title":"Hyperparameter Tuning for Gradient Boosting Frameworks Using Statistical Experimental Design","description":"Applying Advanced Experimental Designs to Optimization of Hyperparameters in XGBoost","date":"2025-05-10","categories":["statistics","bayesian","experimental design"],"image":"images/test_tubes.jpg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}