{
  "hash": "e4569baeba2bd93c8bdaaf94c53b3143",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Hyperparameter Tuning for Gradient Boosting Frameworks Using Statistical Experimental Design\ndescription: \"Applying Advanced Experimental Designs to Optimization of Hyperparameters in XGBoost\"\ndate: 2025-04-10\ncategories: [statistics, bayesian, experimental design]\nimage: \"../assets/img/test_tubes.jpg\"\n---\n\n\n\n\n\n## Introduction/Background\nExperimental design is a crucial part of any statistical experiment, including computer simulations. To explore the eﬀects of eﬃcient design on a computer experiment, we examine three published scientific papers and repeat the papers’ experiments with alternative methods of analysis using advanced experimental designs such as Latin Hypercube Sampling (LHS) and Maximum Projection Designs.\n\nIn the computer simulations discussed here, the experiments at hand are the machine learning models themselves, with the predictive performance of the model serving as the output and the various hyperparameters serving as inputs. A common theme throughout statistical and machine learning research papers -- and all the computational papers we examine here -- is that the authors implement an expensive grid search to optimize the model parameters. However, this approach can be very computationally expensive, both in terms of model run times and computing resources. Moreover, choosing hyperparameters in this way does not attempt to model or account for the relationships between the model performance and hyperparameters. We would like to address these shortcomings in our analysis.\n\nFor this project, we keep a few key goals in mind. First of all, we would like to achieve better predictive performance than what was reported in each of the individual papers, which are geared towards computational physics and chemistry and not super statistically fine-tuned for the utmost optimal model results. Secondly, we look to reduce the estimated run time to save on computational resources. And lastly, we seek to produce a systematic framework with which one can continually improve model performance. As for our methodology, we incorporate principles of computer experiment design in conjunction with Gaussian Process (GP) modeling, with more details presented in the Appendix section below.\n\nIn each paper, the authors use a common method: XGBoost (Chen et al. 2016). For a quick description of the model parameters see (Chen et al. 2018a). In the remainder of this report we discuss each of the investigated research papers in turn, comparing their experimental design methodology and final model results to that of our analysis.\n\n\n## Critical Temperatures of Superconductors\nThe first paper we analyze is “A Data-Driven Statistical Model for Predicting the Critical Temperature of a Superconductor”, published in the Journal of Computational Material Science. The author’s objective in this first paper is to predict at which temperature amaterial becomes a superconductor. A good use case for modeling this relationship is to pre-screen various compounds and find their critical temperatures. There is currently no widely accepted physical theory for the critical temperature of a superconductor, so using a statistical model is a good alternative to model the behavior.[^1]\n\n[^1]: A superconductor is classified as any material which can transport electric charge with no resistance or\nwith no energy loss.\n\nIn the dataset that the author used and provided, the outcome variable is the critical temperature and the predictor variables are various physical properties of the material, such as atomic mass, atomic radius, etc. The data has 21,263 observations in total with 82 columns. For their analysis, the author conducted an exhaustive 198-point grid search over five hyperparameters: learning rate, column subsampling, row subsampling, minimum\nnodes, and maximum depth (Table 1). This amounts to taking all possible combinations of their chosen level settings. For performance evaluation, they used a 25-fold Monte Carlo\ncross-validation (CV) using root mean square error (RMSE) as the performance metric. For each iteration they split the data in two thirds for model fitting and one third for model\nevaluation.\n\n::: {.table-narrow style=\"width: 70%; margin: auto;\"}\n\n\n\n```{table}\n| Parameters         | Levels              |\n|--------------------|---------------------|\n| Learning rate      | 0.010, 0.015, 0.020 |\n| Column subsampling | 0.25, 0.50, 0.75    |\n| Subsample ratio    | 0.5                 |\n| Minimum nodes      | 1, 10               |\n| Maximum depth      | 15, 16, ..., 25     |\n---\ncaption: \"Table 1: Superconductors Hyperparameter Levels\"\n---\n```\n\n\n\n:::\n\nFor our analysis, we construct a Maximum Projection Latin Hypercube with 10 data points (using the MaxPro package in R) and use the same evaluation procedure as the author of the original paper. To transform samples from our design to integers, for parameters such as number of trees and number of leaves, we apply the samples to the inverse CDF of the uniform distribution\nand take the ceiling of the resulting values. To transform the continuous parameters from our design into the appropriate range we can simply shift them by a linear transformation. We repeat this procedure for all three papers.\n\n",
    "supporting": [
      "experimental-design-hyperparameter-tuning_files"
    ],
    "filters": [],
    "includes": {}
  }
}